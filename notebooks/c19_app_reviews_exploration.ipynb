{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NHS C-19 App Reviews Exporation\n",
    "A notebook to explore reviews of the NHS Covid-19 app, and to provide an example notebook for general topic modelling with BERTopic.\n",
    "\n",
    "Specifically, we want to answer the question: are there common themes that arise in Play Store reviews that can be informative to the app product team?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps:\n",
    "- Play back to App team.\n",
    "- Add a step for more structured optimisation of topic modelling, and add more detail on how to do this.\n",
    "- Make more general-purpose, so this notebook can be easily applied to other datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis\n",
    "\n",
    "We will explore the reviews over one year, identify any issues and prepare the data for topic modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re #native\n",
    "import yaml #native\n",
    "import math #native\n",
    "import seaborn as sns \n",
    "from wordcloud import WordCloud, STOPWORDS #issues\n",
    "\n",
    "#topic modelling\n",
    "from sklearn.feature_extraction.text import CountVectorizer #issues\n",
    "from umap import UMAP #issues\n",
    "from hdbscan import HDBSCAN\n",
    "from bertopic import BERTopic \n",
    "from bertopic.vectorizers import ClassTfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pandas options\n",
    "pd.set_option(\"display.max_rows\", 20)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config\n",
    "#when working with github its good store the filepaths in github as cybersecurity dont like, not totally secure, rich says its low risk, \n",
    "# a way around this, its a more abstract way of pointing to the data, obscures it a little bit. this config file is not pushed to the github so its safer\n",
    "#neeed to import yaml package\n",
    "with open(\"../config/config.yaml\", \"r\") as f:\n",
    "        config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set filepaths for reviews files\n",
    "yearmonths = [\n",
    "    \"202111\",\n",
    "    \"202112\",\n",
    "    \"202201\",\n",
    "    \"202202\",\n",
    "    \"202203\",\n",
    "    \"202204\",\n",
    "    \"202205\",\n",
    "    \"202206\",\n",
    "    \"202207\",\n",
    "    \"202208\",\n",
    "    \"202209\",\n",
    "]\n",
    "# INSERT STEM \n",
    "stem_path = config[\"data_folder\"] + \"reviews_reviews_uk.nhs.covid19.production_\" #rhs main bit of file name\n",
    "paths = [f\"{stem_path}{yearmonth}.csv\" for yearmonth in yearmonths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file 202111.csv\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/reviews_reviews_uk.nhs.covid19.production_202111.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\lauren.dunn\\Documents\\DSDU_work\\generalised-nlp-for-survey-datasets\\notebooks\\c19_app_reviews_exploration.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lauren.dunn/Documents/DSDU_work/generalised-nlp-for-survey-datasets/notebooks/c19_app_reviews_exploration.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReading file \u001b[39m\u001b[39m{\u001b[39;00mpath[\u001b[39m-\u001b[39m\u001b[39m10\u001b[39m:]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lauren.dunn/Documents/DSDU_work/generalised-nlp-for-survey-datasets/notebooks/c19_app_reviews_exploration.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/lauren.dunn/Documents/DSDU_work/generalised-nlp-for-survey-datasets/notebooks/c19_app_reviews_exploration.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lauren.dunn/Documents/DSDU_work/generalised-nlp-for-survey-datasets/notebooks/c19_app_reviews_exploration.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         path,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lauren.dunn/Documents/DSDU_work/generalised-nlp-for-survey-datasets/notebooks/c19_app_reviews_exploration.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         engine\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mc\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lauren.dunn/Documents/DSDU_work/generalised-nlp-for-survey-datasets/notebooks/c19_app_reviews_exploration.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         delimiter\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m,\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lauren.dunn/Documents/DSDU_work/generalised-nlp-for-survey-datasets/notebooks/c19_app_reviews_exploration.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         header\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lauren.dunn/Documents/DSDU_work/generalised-nlp-for-survey-datasets/notebooks/c19_app_reviews_exploration.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         encoding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlatin\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lauren.dunn/Documents/DSDU_work/generalised-nlp-for-survey-datasets/notebooks/c19_app_reviews_exploration.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         on_bad_lines\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mwarn\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lauren.dunn/Documents/DSDU_work/generalised-nlp-for-survey-datasets/notebooks/c19_app_reviews_exploration.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         lineterminator\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lauren.dunn/Documents/DSDU_work/generalised-nlp-for-survey-datasets/notebooks/c19_app_reviews_exploration.ipynb#X11sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lauren.dunn/Documents/DSDU_work/generalised-nlp-for-survey-datasets/notebooks/c19_app_reviews_exploration.ipynb#X11sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lauren.dunn/Documents/DSDU_work/generalised-nlp-for-survey-datasets/notebooks/c19_app_reviews_exploration.ipynb#X11sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     temp \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lauren.dunn/Documents/DSDU_work/generalised-nlp-for-survey-datasets/notebooks/c19_app_reviews_exploration.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m         path,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lauren.dunn/Documents/DSDU_work/generalised-nlp-for-survey-datasets/notebooks/c19_app_reviews_exploration.ipynb#X11sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         engine\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mc\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lauren.dunn/Documents/DSDU_work/generalised-nlp-for-survey-datasets/notebooks/c19_app_reviews_exploration.ipynb#X11sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m         lineterminator\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lauren.dunn/Documents/DSDU_work/generalised-nlp-for-survey-datasets/notebooks/c19_app_reviews_exploration.ipynb#X11sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\lauren.dunn\\.conda\\envs\\covid_app_review\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lauren.dunn\\.conda\\envs\\covid_app_review\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lauren.dunn\\.conda\\envs\\covid_app_review\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\lauren.dunn\\.conda\\envs\\covid_app_review\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\lauren.dunn\\.conda\\envs\\covid_app_review\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\lauren.dunn\\.conda\\envs\\covid_app_review\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1736\u001b[0m     f,\n\u001b[0;32m   1737\u001b[0m     mode,\n\u001b[0;32m   1738\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1739\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1740\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1741\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1742\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1743\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1744\u001b[0m )\n\u001b[0;32m   1745\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\lauren.dunn\\.conda\\envs\\covid_app_review\\lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    859\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    860\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    861\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/reviews_reviews_uk.nhs.covid19.production_202111.csv'"
     ]
    }
   ],
   "source": [
    "# Import files\n",
    "for i, path in enumerate(paths):\n",
    "    print(f\"Reading file {path[-10:]}\")\n",
    "    if i == 0:\n",
    "        df = pd.read_csv(\n",
    "            path,\n",
    "            engine=\"c\",\n",
    "            delimiter=\",\",\n",
    "            header=0,\n",
    "            encoding=\"latin\",\n",
    "            on_bad_lines=\"warn\",\n",
    "            lineterminator=\"\\n\",\n",
    "        )\n",
    "    else:\n",
    "        temp = pd.read_csv(\n",
    "            path,\n",
    "            engine=\"c\",\n",
    "            delimiter=\",\",\n",
    "            header=0,\n",
    "            encoding=\"latin\",\n",
    "            on_bad_lines=\"warn\",\n",
    "            lineterminator=\"\\n\",\n",
    "        )\n",
    "        df = pd.concat([df, temp])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "package NameError- anme of app\n",
    "\n",
    "app version\n",
    "- we update over TimeoutError\n",
    "reviewerlangage\n",
    "- lanuge of review\n",
    "\n",
    "\n",
    "device NameError\n",
    "- samsung is called a10\n",
    "- modals of phone\n",
    "\n",
    "\n",
    "dates and times\n",
    "- when review was submitted\n",
    "- if its updated at any point \n",
    "\n",
    "\n",
    "important\n",
    "\n",
    "star rating\n",
    "- every reviedw has star rating\n",
    "- 1-5\n",
    "\n",
    "review text\n",
    "- some reviews with review text\n",
    "\n",
    "\n",
    "developer reply yexty\n",
    "- someone in the app team replies to this\n",
    "\n",
    "\n",
    "cyber security\n",
    "- we dont have to worry about aything as already on playstore\n",
    "- ppi persons name? but didnt seem to be any issues rich queried this months ago\n",
    "\n",
    "\n",
    "\n",
    "NaN\n",
    "- not a number\n",
    "- abscence\n",
    "- ever got to the bottom why we have NaN for \n",
    "\n",
    "\n",
    "ordinal\n",
    "- so we can update version app version number\n",
    "\n",
    "\n",
    "bin raiting\n",
    "- neutral categoires\n",
    "- currently bad vs good\n",
    "- deends what you want to do\n",
    "\n",
    "\n",
    "- some reviews were updated tahts why its flat\n",
    "- month was used by updated column \n",
    "\n",
    "\n",
    "0 is unkonwns (good vs bad over ordinal app version number)\n",
    "\n",
    "if you look at eye more bad reviews at lower app versio nnumber and as they release newer version of the app\n",
    "- maybe a line plot or a stacked bar chart\n",
    "\n",
    "\n",
    "\n",
    "instead of looking at rating\n",
    "- number of records wit ha review\n",
    "- do we see some app versions where people are writing more reviews?\n",
    "- if this version had higher proprotion of negative reviews does that translate to ahving more reviews rate for it\n",
    "    the version of the app is clearly worse than the average\n",
    "    interesting to know if its biased to another. but the rest of them \n",
    "    is 1 version of the app has more reviews \n",
    "    look at average reviews per version\n",
    "\n",
    "\n",
    "\n",
    "that average rating goes upwards. some app versiosn you see a noisy plot. versions seem to be more consistent with \n",
    "\n",
    "\n",
    "\n",
    "date for when app version was released\n",
    "- but do people update it?\n",
    "\n",
    "\n",
    "app version 4\n",
    "- some users who would update it\n",
    "- some users who would not update it 3.9 , god knows who long\n",
    "\n",
    "\n",
    "people could be reviewing version 4, 3.9 and 1. \n",
    "\n",
    "- forced people to move onto latest version\n",
    "\n",
    "\n",
    "\n",
    "person a\n",
    "- version 4 is availabne but they wont update. they will make a review on whatever version they have\n",
    "\n",
    "\n",
    "\n",
    "day of week comparision could be interesting\n",
    "\n",
    "do we see more people more recnerlyt writing reviews. noisness in terms of spikes. proportion of 2. takes spikes out and look at weekly basis. looking at each week. or split by good or bad. more people were writing positive reviews of the app. rolling window over a week.  - lots of stuff you could od with app version number aswel.\n",
    "wary from insight from different version numbers unless we see positive and negative. one version is really bad or really good. the app version number is not as useufl as it might not be. a massive proportion we dont know what version they were on. particaully phone manufactors dont show that info with google\n",
    "\n",
    "or might be particualr users who can set privacy and data sharing etc. why we dont ahve app version for so many records\n",
    "\n",
    "    manaufactors\n",
    "    user driven opt out \n",
    "\n",
    "device name and proprotion of version numbers tat are missing see if theres any correlation there\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "could sikes be bot driven?\n",
    "    bot traffice particaurlly device name. \n",
    "\n",
    "\n",
    "app version and device name could suggest that reviews were not legitimatie \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "would be interesting to see personalty traits and review writing when its not obligatory.\n",
    "\n",
    "wordcloud\n",
    "- could use of ONS training\n",
    "- covered over couple of days\n",
    "- n-grams (grouping together words into words bunched together) - more informative than just 1 word\n",
    "\n",
    "\n",
    "make the bad review word cloud to make it less messy. size of words how prelevant it is in the dataset. needs some work\n",
    "grouping words together before plotting them, do we want ot look at 1* vs 5* reviews. everything in the middle its fine\n",
    "l\n",
    "\n",
    "\n",
    "BERTopic\n",
    "- a lot of same stuff\n",
    "- create sentence embeddings\n",
    "- dimension reduction \n",
    "\n",
    "\n",
    "\n",
    "topic - 1\n",
    "- anything that didnt have a cluster, its all the outliers\n",
    "- lots of different topics that are close together so might not be good\n",
    "- if we have clusters which are talking about the same thing we want to tweak it so its the same\n",
    "- smaller number of topics and each topic is completel different from eachother\n",
    "- each topic to be homogenous with what it contains\n",
    "\n",
    "topic_mode vesctoizer_model\n",
    "- might take a while <30minutes>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "have a look at otpic modalling\n",
    "- look at analysis - how can we make it better in terms of the insights we are getting\n",
    "- how we can make it generic so not putting in specific column names. anywhere analysis is less specific .\n",
    "- look at survey dataset will get us ideas on hwo to change it\n",
    "\n",
    "\n",
    "survey dataset is in the same sharepoint data folder\n",
    "covid-19 dahsboard survey \n",
    "\n",
    "\n",
    "Timelines\n",
    "- no deadlien for this\n",
    "- whenever we get iwth it and whenever \n",
    "\n",
    "\n",
    "apps being decomisied by june\n",
    "- give insight before then \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check files have been imported and appended correctly\n",
    "print(f\"Total number of records: {len(df)}\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill unknown app versions with a new category\n",
    "df[\"App Version Name\"] = df[\"App Version Name\"].fillna(\"UNK (0)\")\n",
    "\n",
    "# Parse ordinal version number to allow sorting more easily\n",
    "df[\"Ordinal App Version Number\"] = [\n",
    "    int(re.findall(r\"\\(.*?\\)\", x)[0][1:-1]) for x in list(df[\"App Version Name\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse dates\n",
    "df[\"Review Submit Date\"] = pd.to_datetime(\n",
    "    pd.to_datetime(df[\"Review Submit Date and Time\"]).dt.date\n",
    ")\n",
    "df[\"Review Last Updated Date\"] = pd.to_datetime(\n",
    "    pd.to_datetime(df[\"Review Last Update Date and Time\"]).dt.date\n",
    ")\n",
    "df[\"Review Submit Month\"] = df[\"Review Submit Date\"].dt.strftime(\"%Y-%m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a binary 'good/bad' category from Star Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary \"Good/Bad\" rating\n",
    "def bin_rating(x):\n",
    "    if x <= 3:\n",
    "        return \"Bad\"\n",
    "    elif x > 3:\n",
    "        return \"Good\"\n",
    "    else:\n",
    "        return \"Missing\"\n",
    "\n",
    "\n",
    "df[\"Binary Rating\"] = df[\"Star Rating\"].apply(bin_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_axs(nrows: int, ncols: int, figsize=(12, 5), sharex=\"none\", sharey=\"none\"):\n",
    "    \"\"\"A function to generate a figure and set of axes for subplots.\"\"\"\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=nrows, ncols=ncols, figsize=figsize, sharex=sharex, sharey=sharey\n",
    "    )\n",
    "    return fig, axes.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of reviews per star rating\n",
    "ratings_df = df.groupby(df[\"Star Rating\"]).count()[\"Package Name\"]\n",
    "ratings_with_review_df = df.groupby(df[\"Star Rating\"]).count()[\"Review Text\"]\n",
    "num_records = len(df)\n",
    "\n",
    "fig, axs = generate_axs(1, 2)\n",
    "\n",
    "(ratings_df / num_records).plot(\n",
    "    kind=\"bar\",\n",
    "    title=f\"Distribution of ratings across {num_records} total ratings\",\n",
    "    ax=axs[0],\n",
    ")\n",
    "(ratings_with_review_df / num_records).plot(\n",
    "    kind=\"bar\",\n",
    "    title=f\"Proportion of records with a review out of {num_records} total ratings, by rating\",\n",
    "    ax=axs[1],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that, across the year, around 60% of ratings are 5/5, with around 18% 1/5 and the rest distributed between the two. When we look at ratings where a review has also been provided, we see the majority of these reviews come with either a 5/5 or 1/5 rating, and they over-index on 1/5 ratings significantly. We might expect users with stronger feelings are more likely to leave a review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_ratings_date_df = (\n",
    "    df.groupby(by=[\"Binary Rating\", \"Review Submit Month\"]).count().reset_index()\n",
    ")\n",
    "bin_ratings_version_df = (\n",
    "    df.groupby(by=[\"Binary Rating\", \"Ordinal App Version Number\"]).count().reset_index()\n",
    ")\n",
    "\n",
    "fig, axs = generate_axs(1, 2, figsize=(20, 8))\n",
    "\n",
    "\n",
    "sns.lineplot(\n",
    "    data=bin_ratings_date_df,\n",
    "    x=\"Review Submit Month\",\n",
    "    y=\"Package Name\",\n",
    "    hue=\"Binary Rating\",\n",
    "    ax=axs[0],\n",
    ")\n",
    "sns.barplot(\n",
    "    data=bin_ratings_version_df,\n",
    "    x=\"Ordinal App Version Number\",\n",
    "    y=\"Package Name\",\n",
    "    hue=\"Binary Rating\",\n",
    "    ax=axs[1],\n",
    ")\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "    ax.set_ylabel(\"Count of ratings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the proportion of reviews that are positive (>3) increases substantially from June-22 onwards. W also see that the proportion of positive reviews for newer versions is higher than for older or unknown version numbers. Again, this is to be expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reviews by version\n",
    "versions_with_review_df = (\n",
    "    df.groupby([df[\"App Version Name\"], df[\"Ordinal App Version Number\"]])\n",
    "    .count()[[\"Star Rating\", \"Review Text\"]]\n",
    "    .sort_values(by=\"Ordinal App Version Number\")\n",
    ")\n",
    "versions_with_review_df.plot(\n",
    "    kind=\"bar\",\n",
    "    title=f\"Number of records with a review, by app version\",\n",
    "    figsize=(10, 5),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avg rating by version, for versions with more than X ratings\n",
    "versions_with_rating_df = df.groupby(df[\"Ordinal App Version Number\"]).count()\n",
    "\n",
    "versions = versions_with_rating_df[\n",
    "    versions_with_rating_df[\"Star Rating\"] > 5\n",
    "].reset_index()\n",
    "\n",
    "ratings_by_version_df = (\n",
    "    df[\n",
    "        df[\"Ordinal App Version Number\"].isin(\n",
    "            list(versions[\"Ordinal App Version Number\"])\n",
    "        )\n",
    "    ]\n",
    "    .groupby([\"Ordinal App Version Number\", \"App Version Name\"])\n",
    "    .mean()[\"Star Rating\"]\n",
    "    .sort_index()\n",
    "    .plot(\n",
    "        kind=\"bar\",\n",
    "        figsize=(10, 4),\n",
    "        title=\"Avg rating by version, for versions with >5 ratings\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see average ratings being higher for more recent versions of the app, which is to be expected given issues may have been addressed in the intervening releases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution by review submitted date\n",
    "reviews_by_datetime_df = df.groupby(df[\"Review Last Updated Date\"]).count()[\n",
    "    [\"Star Rating\", \"Review Text\"]\n",
    "]\n",
    "reviews_by_datetime_df.plot(\n",
    "    kind=\"line\", title=\"Ratings by day last updated\", figsize=(12, 6)\n",
    ")\n",
    "\n",
    "reviews_by_datetime_df[\"Reviews Per Rating\"] = (\n",
    "    reviews_by_datetime_df[\"Review Text\"] / reviews_by_datetime_df[\"Star Rating\"]\n",
    ")\n",
    "print(f'Mean reviews per rating: {reviews_by_datetime_df[\"Reviews Per Rating\"].mean()}')\n",
    "\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do the reviews on spike days consist of? What could explain those spikes?\n",
    "reviews_by_datetime_df.sort_values(by=\"Star Rating\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is causing spikes on particular dates? Check their distribution by star rating, version, device and check the text of some reviews. Could they be artificial, or is it due to mass update to new version?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ratings for days where spikes occur\n",
    "for col in [\"Device\", \"App Version Name\"]:\n",
    "    df_subset = df.loc[\n",
    "        df[\"Review Last Updated Date\"].isin(\n",
    "            [\n",
    "                pd.to_datetime(\"2022-06-09\"),\n",
    "                pd.to_datetime(\"2022-06-10\"),\n",
    "                pd.to_datetime(\"2022-09-28\"),\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "    df_subset.groupby(by=[col, \"Binary Rating\"]).size().unstack().plot(\n",
    "        kind=\"bar\",\n",
    "        stacked=True,\n",
    "        figsize=(14, 6),\n",
    "        title=f\"Count of ratings by binary rating and {col}\",\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "print(df_subset[\"Review Text\"].dropna().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dates 2022-06-09 and 2022-06-10 conincide with an orchestrated forced upgrate from old versions. Distribution of star ratings is more positive than average across all reviews, possibly reflecting a more stable version due to successive bug fixes and a better user experience generally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modelling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using BERTopic to model common topics\n",
    "[BERTopic](https://maartengr.github.io/BERTopic/algorithm/algorithm.html) brings together a range of techniques and open-source libraries to yield an appropriate topic mapping for each document in a corpus. The explanation of the topic modeling process is below.\n",
    "\n",
    "- Word embeddings using Bert - “SentenceTransformer” package using “distilbert-base-nli-stsb-mean-tokens” embeddings by default (768-dimensional vector representation of the review).\n",
    "- Dimensionality reduction with “Umap” or Unification Map (PCA and Truncated SVD also available).\n",
    "- HDBSCAN for clustering the post-Umap embeddings (kMeans and BIRCH also available).\n",
    "- [\"c-TF IDF\"](https://maartengr.github.io/BERTopic/getting_started/ctfidf/ctfidf.html#c-tf-idf) (class-based TF-IDF) process after clustering the documents. This allows us to extract the most used mutual words for every cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review Text, dropping nulls.\n",
    "df_topic = df[[\"Review Text\", \"Review Submit Date\"]].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate word clouds for good and bad reviews, to see what the most common words are in these reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wordcloud(df):\n",
    "    comment_words = \"\"\n",
    "    stopwords = set(STOPWORDS)\n",
    "\n",
    "    # iterate through the dataframe\n",
    "    for val in df[\"Review Text\"]:\n",
    "\n",
    "        # typecaste each val to string\n",
    "        val = str(val)\n",
    "        # split the value\n",
    "        tokens = val.split()\n",
    "\n",
    "        # Converts each token into lowercase\n",
    "        for i in range(len(tokens)):\n",
    "            tokens[i] = tokens[i].lower()\n",
    "            comment_words += \" \".join(tokens) + \" \"\n",
    "\n",
    "    for j in [\"nan\", \"app\", \"covid\"]:\n",
    "        comment_words = comment_words.replace(j, \"\")\n",
    "\n",
    "    wordcloud = WordCloud(\n",
    "        width=600,\n",
    "        height=600,\n",
    "        background_color=\"white\",\n",
    "        stopwords=stopwords,\n",
    "        min_font_size=10,\n",
    "    ).generate(comment_words)\n",
    "\n",
    "    # plot the WordCloud image\n",
    "    plt.figure(figsize=(8, 8), facecolor=None)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad=0)\n",
    "\n",
    "    plt.show()\n",
    "    return wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bin in [\"Good\", \"Bad\"]:\n",
    "    print(f\"Wordcloud for {bin} reviews\")\n",
    "    wordcloud = create_wordcloud(df=df.loc[df[\"Binary Rating\"] == bin])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"From a total of {len(df)} ratings, {len(df_topic)} have review text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep data for modelling\n",
    "docs = df_topic[\"Review Text\"].reset_index().drop(columns=\"index\").to_numpy().ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERTopic modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now instantiate the BERTopic model and use some other consituent classes such as UMAP and HDBSCAN for further parameterisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Instantiate  UMAP, HDBSCAN and cTF-IDF to pass them to BERTopic.\n",
    "\n",
    "# umap_model = UMAP(n_neighbors=20, n_components=10, metric='cosine', low_memory=False)\n",
    "# hdbscan_model = HDBSCAN(min_cluster_size=10, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "ctfidf_model = ClassTfidfTransformer(bm25_weighting=True, reduce_frequent_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sklearn CountVectorizer to remove stopwords after having generated embeddings, and train model\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "# Train BERTopic model\n",
    "topic_model = BERTopic(\n",
    "    language=\"multilingual\",\n",
    "    ctfidf_model=ctfidf_model,\n",
    "    # umap_model=umap_model,\n",
    "    # hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    calculate_probabilities=True,\n",
    "    verbose=True,\n",
    "    nr_topics=\"auto\",\n",
    "    #  low_memory = True,\n",
    "    top_n_words=6,\n",
    "    # min_topic_size = 15,\n",
    "    # diversity=0.5\n",
    ")\n",
    "topics, probs = topic_model.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show topic distribution of largest n topics\n",
    "freq = topic_model.get_topic_info()\n",
    "freq.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of topics generated: {len(freq)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate topic allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes: Parameter optimisation\n",
    "\n",
    "Optimising topic modelling with BERTopic can be challenging. Between the cTF-IDF, dimensionality reduction and clustering algorithms, there is a very large hyperparameter space to search over. Also, visual methods for evaluating topic assignment require the application of dimensionality reduction methods to word embeddings. Therefore it can be difficult to understand which set of hyperparameters yields the optimal result.\n",
    "\n",
    "Below is a set of observations based on experimentation with a few key hyperparameters. All comparisons are based on an instance with the default parameter values.\n",
    "\n",
    "- BERTopic parameters:\n",
    "    - nr_topics: Number of topics. Setting this to \"auto\" results in far fewer topics being generated, with many more observations classed as the largest topic and far fewer outliers. It also seems to result in topics with a small intertopic distance, but the 'top words' of those nearby topics are not consistent (e.g. 1_awful_rubbish_terrible_super and 7_excellent_perfect_fantastic_awesome are nearby when mapped to 2D).\n",
    "    - min_topic_size: Minimum number of documents in a topic. When this is set to 20 we see a larger largest topic, with still many outliers. Half as many topics are generated versus when this is set to a default of 10.\n",
    "    \n",
    "\n",
    "- cTF-IDF parameters:\n",
    "\n",
    "    - bm25_weighting=True will use a class-based BM-25 weighting measure instead of the default method for calculating the importance score per word in each topic. For smaller datasets, this measure can be more robust to stop words that appear in the data. In this case, it results in far fewer outliers, and a much larger largest topic.\n",
    "    - reduce_frequent_words=True takes the square root of the term frequency after applying the weighting scheme. This reduces the effect of very frequent words in each topic. In this case, this parameter has a large effect, creating more outliers, a smaller largest topic and more evenly sized topics. This is likely due to the reduced effect of common words in topics, such as 'good', 'app', 'covid' etc.\n",
    "\n",
    "- UMAP parameters:\n",
    "\n",
    "    - n_neighbors is the numer of neighboring sample points used when making the manifold approximation. Increasing this value typically results in a more global view of the embedding structure whilst smaller values result in a more local view. Increasing this value often results in larger clusters being created. n_neighbours=5 results in the same number of topics (c.30) but they are more even in size (c.50), with a very large set of outliers (~370). At n_neighbours=5 we get slightly larger largest topics, with n_neighbours=20 we get a larger 'largest' topic.\n",
    "    - n_components refers to the dimensionality of the embeddings after reducing them. This is set as a default to 5 in order to reduce dimensionality as much as possible whilst trying to maximize the information kept in the resulting embeddings. Although lowering or increasing this value has an influence on the quality of embeddings, its effect is largest on the performance of HDBSCAN.\n",
    "    \n",
    "- HDBSCAN:\n",
    "    - min_cluster_size: See min_topic_size.\n",
    "    - metric: The measure to use when evaluating clusters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eyeball a selection of reviews to check for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe with review test, topics and topic frequencies\n",
    "def set_topics_to_reviews(df, freq, topics):\n",
    "    df_1 = pd.DataFrame(df[\"Review Text\"].dropna())\n",
    "    df_1[\"Topic\"] = topics\n",
    "    df_2 = pd.merge(left=df_1, right=df)[\n",
    "        [\n",
    "            \"Review Text\",\n",
    "            \"Topic\",\n",
    "            \"Star Rating\",\n",
    "            \"App Version Name\",\n",
    "            \"App Version Code\",\n",
    "            \"Binary Rating\",\n",
    "        ]\n",
    "    ]\n",
    "    df_2 = pd.merge(left=df_2, right=freq, on=\"Topic\").drop_duplicates()\n",
    "    return df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = set_topics_to_reviews(df, freq, topics)\n",
    "\n",
    "topic_number = 0\n",
    "df_3.loc[df_3[\"Topic\"] == topic_number].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise distribution of star rating for n largest topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise most common topics by star rating\n",
    "\n",
    "\n",
    "def plot_topic_ratings(df, n_topics_to_viz):\n",
    "\n",
    "    # define grid of plots\n",
    "    nrows = 1 + math.ceil(n_topics_to_viz / 3)\n",
    "\n",
    "    fig, axs = generate_axs(nrows=nrows, ncols=3, figsize=(18, 15), sharex=False)\n",
    "\n",
    "    # Plot star rating distribution across all topics\n",
    "    df_rating_all_topics = (\n",
    "        df.drop(\n",
    "            columns=[\"Count\", \"App Version Code\", \"App Version Name\", \"Topic\", \"Name\"]\n",
    "        )\n",
    "        .groupby(by=[\"Star Rating\"])\n",
    "        .count()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    df_rating_all_topics.plot(\n",
    "        kind=\"bar\",\n",
    "        x=\"Star Rating\",\n",
    "        y=\"Review Text\",\n",
    "        title=\"All topics\",\n",
    "        ax=axs[0],\n",
    "        label=\"_nolegend_\",\n",
    "    )\n",
    "\n",
    "    df_topic_rating = (\n",
    "        df.loc[df[\"Topic\"] < n_topics_to_viz]\n",
    "        .groupby(by=[\"Topic\", \"Name\", \"Star Rating\"])\n",
    "        .count()\n",
    "        .drop(columns=[\"Count\", \"App Version Code\", \"App Version Name\"])\n",
    "        .reset_index()\n",
    "    )\n",
    "    for top in range(-1, n_topics_to_viz):\n",
    "        df_topic_rating.loc[df_topic_rating[\"Topic\"] == top].plot(\n",
    "            kind=\"bar\",\n",
    "            x=\"Star Rating\",\n",
    "            y=\"Review Text\",\n",
    "            title=f\"Topic {top}\",\n",
    "            ax=axs[top + 2],\n",
    "            label=\"_nolegend_\",\n",
    "        )\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics_to_viz = 14\n",
    "plot_topic_ratings(df_3, n_topics_to_viz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plots show how each topic is distributed by star rating, and therefore how homogenous a topic is in its sentiment. We can expect a 'good' set of topics to be consistent in their sentiment, i.e. to be less polarised in star ratings, and to look less like the reference distribution of star ratings across the whole dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise the intertopic distance to see how document embeddings and their topics are distributed in 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of well-defined topics should yield large inter-topic distances. This visualisation also allows us to see which topics are semantically similar. However, 2D representations of document embeddings should only be used as a guide to the quality of topic clustering, as a lot of information is lost when projecting word embeddings down to this dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see how document embeddings are distributed in 2D, with their colour representing the assigned topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_topics = topic_model.hierarchical_topics(docs)\n",
    "topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wish to reduce the number of topics in our model, visualising the hierarchy of topics (using scipy.cluster.hierarchy) can be useful. The .reduce_topics() method can be used to reduce to a specified number of topics. You can also use the merge_topics() method to merge topics if the above visuals suggest they are very similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualise the word scores per topic. This represents the importance of words in each topic and is a concise way to understand what each topic pertains to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_barchart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics_over_time = topic_model.topics_over_time(docs, topics, df_topic[\"Review Submit Date\"].dt.strftime('%Y/%m/%d').to_list(), datetime_format='%Y/%m/%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also reduce the number of topics using the reduce_topics() method. The number of topics is hard-coded, and can be inferred from the above visualisations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nr_topics = 7\n",
    "# topic_model.reduce_topics(docs, nr_topics=nr_topics)\n",
    "# topics_reduced, probs_reduced = topic_model.transform(docs)\n",
    "# freq_reduced = topic_model.get_topic_info()\n",
    "# df_3_reduced = set_topics_to_reviews(df, freq_reduced, topics_reduced)\n",
    "# plot_topic_ratings(df_3_reduced, nr_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most common topics by rating\n",
    "\n",
    "Let's look at the top n topics by our binary rating (>/<= 3). We'll do this by:\n",
    "- Finding most common topics by good/bad.\n",
    "- Seeing how well split those topics are between good and bad reviews.\n",
    "- Printing examples of the text for manual evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bin = (\n",
    "    df_3.loc[df_3[\"Topic\"] > -1]\n",
    "    .groupby(by=[\"Binary Rating\", \"Topic\", \"Name\"])\n",
    "    .count()[\"Review Text\"]\n",
    "    .reset_index()\n",
    "    .sort_values(by=[\"Binary Rating\", \"Review Text\"], ascending=False)\n",
    ")\n",
    "\n",
    "output_cols = [\n",
    "    \"Name\",\n",
    "    \"Review Text\",\n",
    "    \"Star Rating\",\n",
    "    \"App Version Name\",\n",
    "    \"App Version Code\",\n",
    "    \"Binary Rating\",\n",
    "]\n",
    "\n",
    "output = df_3.iloc[0:0][output_cols]\n",
    "\n",
    "# Find top 3 topics for good and bad reviews\n",
    "for bin in [\"Good\", \"Bad\"]:\n",
    "    top_n_topics = df_bin.loc[df_bin[\"Binary Rating\"] == bin].nlargest(\n",
    "        3, [\"Review Text\"]\n",
    "    )\n",
    "    for topic_name in top_n_topics[\"Name\"]:\n",
    "        temp = df_3.loc[df_3[\"Name\"] == topic_name][output_cols]\n",
    "        output = output.append(temp)\n",
    "\n",
    "        print(f\"Plotting for {bin} reviews, {topic_name} topic...\\n\\n\")\n",
    "        wordcloud = create_wordcloud(df=temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see clear themes in these topics. For example, we see the most common 'good' topic is around keeping people safe (topic 0_great_safe_glad_definitely). We see some bad reviews related to self isolation period and symptoms (topic 4_10_11_isolation_self), close contacts and lfd tests (5_registered_register_ping_id) and another on downloads, technical probelms and updates (topic 6_pending_error_download_tried).\n",
    "\n",
    "We can print the first n reviews per topic, per sentiment, to see some examples of each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = (\n",
    "    output.groupby([\"Binary Rating\", \"Name\"])\n",
    "    .head(5)\n",
    "    .reset_index(drop=True)\n",
    "    .sort_values([\"Binary Rating\", \"Name\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None)\n",
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise topic by version\n",
    "fig, axs = generate_axs(2, 1, figsize=(20, 20))\n",
    "\n",
    "sns.barplot(\n",
    "    data=output.groupby([\"App Version Name\", \"App Version Code\", \"Name\"])\n",
    "    .count()[\"Star Rating\"]\n",
    "    .reset_index(),\n",
    "    x=\"App Version Code\",\n",
    "    y=\"Star Rating\",\n",
    "    hue=\"Name\",\n",
    "    ax=axs[0],\n",
    ")\n",
    "axs[0].set_ylabel(\"Number of ratings\")\n",
    "axs[0].set_xticklabels(\n",
    "    axs[0].get_xticklabels(), rotation=45, horizontalalignment=\"right\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "faa113d29ee7b1dc6ee8ff65de44c1e3a29db0f8d3a5975087728aa8e521ff23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
